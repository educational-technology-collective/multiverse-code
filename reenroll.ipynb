{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "6427cca6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from imblearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import NearMiss\n",
    "from fairlearn.metrics import MetricFrame, equalized_odds_difference\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "from joblib import Parallel, delayed, Memory\n",
    "import os\n",
    "\n",
    "from jupyter_server.services.config import ConfigManager\n",
    "\n",
    "cm = ConfigManager()\n",
    "cm.update('notebook', {\"ServerApp.iopub_msg_rate_limit\": 100000})\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "%run helper.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a27ad2bf-91d8-4a66-820b-b96f9e26e533",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "term_info_df = pd.read_csv(r\"/data0/larc/LARC Student_LARC_20230125_STDNT_TERM_INFO.csv\", low_memory=False)\n",
    "student_info_df = pd.read_csv(\"/data0/larc/LARC Student_LARC_20230125_STDNT_INFO.csv\", low_memory=False)\n",
    "# class_info_df = pd.read_csv(\"/data0/larc/LARC Student_LARC_20230125_STDNT_TERM_CLASS_INFO.csv\", low_memory=False)\n",
    "transfer_info_df = pd.read_csv(\"/data0/larc/LARC Student_LARC_20230125_STDNT_TERM_TRNSFR_INFO.csv\", low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "c488ae9f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------SEX----------------\n",
      "STDNT_SEX_SHORT_DES\n",
      "Female    54656\n",
      "Male      54738\n",
      "dtype: int64\n",
      "---------------RACE---------------\n",
      "STDNT_ETHNC_GRP_SHORT_DES\n",
      "2 or More      4418\n",
      "Asian         22280\n",
      "Black          4645\n",
      "Hawaiian         50\n",
      "Hispanic       6361\n",
      "Native Amr      154\n",
      "Not Indic      6369\n",
      "White         65117\n",
      "dtype: int64\n",
      "---------------SEX/Race------------\n",
      "Sex/Race\n",
      "Female/2 or More      2370\n",
      "Female/Asian         10254\n",
      "Female/Black          2689\n",
      "Female/Hawaiian         21\n",
      "Female/Hispanic       3323\n",
      "Female/Native Amr       79\n",
      "Female/Not Indic      2866\n",
      "Female/White         33054\n",
      "Male/2 or More        2048\n",
      "Male/Asian           12026\n",
      "Male/Black            1956\n",
      "Male/Hawaiian           29\n",
      "Male/Hispanic         3038\n",
      "Male/Native Amr         75\n",
      "Male/Not Indic        3503\n",
      "Male/White           32063\n",
      "dtype: int64\n",
      "---------------FG-----------------\n",
      "IS_FIRST_GEN\n",
      "0    101573\n",
      "1      7821\n",
      "dtype: int64\n",
      "--------------Income--------------\n",
      "EST_GROSS_FAM_INC_DES\n",
      "                       20409\n",
      "$100,000 - $149,999    12460\n",
      "$150,000 - $199,999     7937\n",
      "$25,000 - $49,999       8479\n",
      "$50,000 - $74,999       8572\n",
      "$75,000 - $99,999       8638\n",
      "Don't Know              4508\n",
      "Less than $25,000       5741\n",
      "More than $100,000      7865\n",
      "More than $200,000     21809\n",
      "dtype: int64\n",
      "---------------FG/Income------------\n",
      "FG/Income\n",
      "0/                       19545\n",
      "0/$100,000 - $149,999    12094\n",
      "0/$150,000 - $199,999     7819\n",
      "0/$25,000 - $49,999       6237\n",
      "0/$50,000 - $74,999       7354\n",
      "0/$75,000 - $99,999       8053\n",
      "0/Don't Know              4313\n",
      "0/Less than $25,000       3855\n",
      "0/More than $100,000      7755\n",
      "0/More than $200,000     21572\n",
      "0/nan                     2976\n",
      "1/                         864\n",
      "1/$100,000 - $149,999      366\n",
      "1/$150,000 - $199,999      118\n",
      "1/$25,000 - $49,999       2242\n",
      "1/$50,000 - $74,999       1218\n",
      "1/$75,000 - $99,999        585\n",
      "1/Don't Know               195\n",
      "1/Less than $25,000       1886\n",
      "1/More than $100,000       110\n",
      "1/More than $200,000       237\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Print First-Year and Trasnfer Make Up\n",
    "relevant_student_df = get_student_df(student_info_df, include_transfer=True, include_nan=False, term_cut=\"FA 2022\")\n",
    "add_first_gen_df = add_first_gen_col(relevant_student_df)\n",
    "add_age_df = add_age_col(add_first_gen_df)\n",
    "add_transfer_credit_df = add_transfer_credits_col(add_age_df, transfer_info_df)\n",
    "add_max_sat_df = add_max_sat_col(add_transfer_credit_df)\n",
    "add_major_df = add_major_col(add_max_sat_df, term_info_df)\n",
    "add_cum_gpa_df = add_cum_gpa_col(add_major_df, term_info_df)\n",
    "add_credits_taken_df = add_credits_taken_col(add_cum_gpa_df, term_info_df)\n",
    "add_credits_taken_df['Missing_Income'] = add_credits_taken_df['EST_GROSS_FAM_INC_DES'].isna().astype(int)\n",
    "\n",
    "add_credits_taken_df['Sex/Race'] = (add_credits_taken_df['STDNT_SEX_SHORT_DES'] + '/' + add_credits_taken_df['STDNT_ETHNC_GRP_SHORT_DES']).astype(str)\n",
    "add_credits_taken_df['FG/Income'] = (\n",
    "    add_credits_taken_df['IS_FIRST_GEN'].astype(str).str.strip() + \n",
    "    '/' + \n",
    "    add_credits_taken_df['EST_GROSS_FAM_INC_DES'].astype(str).str.strip()\n",
    ")\n",
    "\n",
    "add_features_df = add_credits_taken_df.copy()\n",
    "\n",
    "sex_group = add_features_df.groupby('STDNT_SEX_SHORT_DES').size()\n",
    "print(\"---------------SEX----------------\")\n",
    "print(sex_group)\n",
    "print(\"---------------RACE---------------\")\n",
    "race_group = add_features_df.groupby(\"STDNT_ETHNC_GRP_SHORT_DES\").size()\n",
    "print(race_group)\n",
    "\n",
    "sex_race_group = add_features_df.groupby('Sex/Race').size()\n",
    "print(\"---------------SEX/Race------------\")\n",
    "print(sex_race_group)\n",
    "\n",
    "\n",
    "fg_group = add_features_df.groupby('IS_FIRST_GEN').size()\n",
    "print(\"---------------FG-----------------\")\n",
    "print(fg_group)\n",
    "\n",
    "income_group = add_features_df.groupby(\"EST_GROSS_FAM_INC_DES\").size()\n",
    "print(\"--------------Income--------------\")\n",
    "print(income_group)\n",
    "\n",
    "\n",
    "fg_income_group = add_features_df.groupby(\"FG/Income\").size()\n",
    "print(\"---------------FG/Income------------\")\n",
    "print(fg_income_group)\n",
    "\n",
    "# add_features_df = fill_income_col(add_features_df)\n",
    "# add_features_df.to_csv('add_features.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "ebf1cee4-1325-4a3a-8f6b-65d7fc95463e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# reenroll_multiverse_col_names = [\"Include_Transfer\", \"Include_Covid\", \"Include_Sex\", \"Include_Race\",  \"Train_Size\", \"Handle_Nan\", \n",
    "#                                \"Scaler\", \"Encoder\", \"Sampler\", \"Classifier\", \"N_Estimators\", \"Learning_Rate\", \"C\", \"AUC\", \"Equalized_Odds_Difference_Sex/Race\",\n",
    "#                                 \"Equalized_Odds_Difference_FG/Income\"]\n",
    "# reenroll_multiverse_df = pd.DataFrame(columns=reenroll_multiverse_col_names)                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "fd5b15ba-1ae7-4e99-9bc7-98bbf90cfd83",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "add_first_gen_df = add_first_gen_col(student_info_df)\n",
    "add_age_df = add_age_col(add_first_gen_df)\n",
    "add_transfer_credit_df = add_transfer_credits_col(add_age_df, transfer_info_df)\n",
    "add_max_sat_df = add_max_sat_col(add_transfer_credit_df)\n",
    "add_major_df = add_major_col(add_max_sat_df, term_info_df)\n",
    "add_cum_gpa_df = add_cum_gpa_col(add_major_df, term_info_df)\n",
    "add_credits_taken_df = add_credits_taken_col(add_cum_gpa_df, term_info_df)\n",
    "add_credits_taken_df['Missing_Income'] = add_credits_taken_df['EST_GROSS_FAM_INC_DES'].isna().astype(int)\n",
    "\n",
    "add_credits_taken_df['Sex/Race'] = (add_credits_taken_df['STDNT_SEX_SHORT_DES'] + '/' + add_credits_taken_df['STDNT_ETHNC_GRP_SHORT_DES']).astype(str)\n",
    "add_credits_taken_df['FG/Income'] = (\n",
    "    add_credits_taken_df['IS_FIRST_GEN'].astype(str).str.strip() + \n",
    "    '/' + \n",
    "    add_credits_taken_df['EST_GROSS_FAM_INC_DES'].astype(str).str.strip()\n",
    ")\n",
    "add_features_df = add_credits_taken_df.copy()\n",
    "add_features_df = fill_income_col(add_features_df)\n",
    "add_features_df.to_csv(\"add_features.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "48b5a53b-3c86-41ae-ba28-64f5170d81c0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "add_features_df = pd.read_csv(\"add_features.csv\", low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "e953bdc7-201f-4038-b4ae-10dd839f6e01",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "reenroll_multiverse_col_names = [\"Include_Transfer\", \"Include_Covid\", \"Include_Sex\", \"Include_Race\",  \"Train_Size\", \"Handle_Nan\", \n",
    "                               \"Scaler\", \"Encoder\", \"Sampler\", \"Classifier\", \"N_Estimators\", \"Learning_Rate\", \"C\", \"AUC\", \"Equalized_Odds_Difference_Sex\", \"Equalized_Odds_Difference_Race\", \"Equalized_Odds_Difference_FG\",\n",
    "                                \"Equalized_Odds_Difference_Income\", \"Equalized_Odds_Difference_Sex/Race\",\n",
    "                                \"Equalized_Odds_Difference_FG/Income\"]\n",
    "feature_columns = ['STDNT_INTL_IND', 'STDNT_NTV_ENG_SPKR_IND', 'FIRST_US_PRMNNT_RES_PSTL_5_CD', 'PRNT_MAX_ED_LVL_DES', 'FIRST_TERM_ATTND_SHORT_DES',\n",
    "                      'STARTING_AGE', 'IS_FIRST_GEN', 'TRANSFER_CREDITS', 'MAX_SAT_SCR', 'EST_GROSS_FAM_INC_DES', 'Missing_Income', 'FIRST_YR_CUM_GPA', \n",
    "                      'FIRST_YR_TAKEN_CREDITS', 'HS_GPA', 'HS_CALC_IND', 'HS_CHEM_LAB_IND', 'PGM_1_MAJOR_1_CIP_DES', 'FIRST_UG_ENTRY_TYP_DES', 'STDNT_SEX_SHORT_DES',\n",
    "                  'STDNT_ETHNC_GRP_SHORT_DES', 'Sex/Race', 'FG/Income']\n",
    "\n",
    "\n",
    "def process_combination(include_transfer, include_covid, handle_nan, train_size, include_sex, include_race, encoder, scaler, sampler, classifier, classifier_params):\n",
    "    pipe = Pipeline(steps=[\n",
    "        (\"Transformer\", 'passthrough'),\n",
    "        (\"Scaler\", 'passthrough'),\n",
    "        (\"Sampler\", 'passthrough'),\n",
    "        (\"Classifier\", 'passthrough')\n",
    "    ]) \n",
    "    \n",
    "    # Load and preprocess data\n",
    "    premature_X, premature_Y = premature(add_features_df, term_info_df, include_transfer=include_transfer, include_covid=include_covid)\n",
    "    features_df = premature_X[feature_columns].reset_index(drop=True)\n",
    "    premature_Y = premature_Y.reset_index(drop=True)\n",
    "    \n",
    "    if handle_nan == 'Drop':\n",
    "        rows_with_nan = features_df.isnull().any(axis=1)\n",
    "        X_after_nan = features_df[~rows_with_nan]\n",
    "        Y_after_nan = premature_Y[~rows_with_nan]\n",
    "        X_train, X_test, Y_train, Y_test = train_test_split(X_after_nan, Y_after_nan, train_size=train_size) # Train-Test Split\n",
    "    else:  \n",
    "        X_train, X_test, Y_train, Y_test = train_test_split(features_df, premature_Y, train_size=train_size) # Train - Test Split first\n",
    "        X_train = X_train.reset_index(drop=True)\n",
    "        X_test = X_test.reset_index(drop=True)\n",
    "        Y_train = Y_train.reset_index(drop=True)\n",
    "        Y_test = Y_test.reset_index(drop=True)\n",
    "\n",
    "        # Perform imputation on the training data and apply it to the test data\n",
    "        imputer = SimpleImputer(strategy=\"most_frequent\")\n",
    "        X_train_array = imputer.fit_transform(X_train)\n",
    "        X_train = pd.DataFrame(X_train_array, columns=X_train.columns, index=X_train.index)\n",
    "\n",
    "        X_test_array = imputer.transform(X_test)\n",
    "        X_test = pd.DataFrame(X_test_array, columns=X_test.columns, index=X_test.index)\n",
    "\n",
    "    sex_test = X_test['STDNT_SEX_SHORT_DES']\n",
    "    race_test = X_test['STDNT_ETHNC_GRP_SHORT_DES']\n",
    "    sex_race_test = X_test['Sex/Race']\n",
    "    fg_test = X_test['IS_FIRST_GEN']\n",
    "    income_test = X_test['EST_GROSS_FAM_INC_DES']\n",
    "    fg_income_test = X_test['FG/Income']\n",
    "    \n",
    "    \n",
    "    X_train = X_train.drop(columns=['Sex/Race', 'FG/Income'])\n",
    "    X_test = X_test.drop(columns=['Sex/Race', 'FG/Income'])\n",
    "    # Handle categorical features\n",
    "    if not include_sex:\n",
    "        X_train = X_train.drop(columns=['STDNT_SEX_SHORT_DES'])\n",
    "        X_test = X_test.drop(columns=['STDNT_SEX_SHORT_DES'])\n",
    "    if not include_race:\n",
    "        X_train = X_train.drop(columns=['STDNT_ETHNC_GRP_SHORT_DES'])\n",
    "        X_test = X_test.drop(columns=['STDNT_ETHNC_GRP_SHORT_DES'])\n",
    "\n",
    "    # Setup pipeline with caching\n",
    "    categorical_columns = X_train.select_dtypes(include=['object']).columns\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('cat', encoder, categorical_columns)\n",
    "        ],\n",
    "        remainder='passthrough'\n",
    "    )\n",
    "\n",
    "    \n",
    "    classifier.set_params(**classifier_params)\n",
    "    pipe.set_params(**{\n",
    "        \"Transformer\": preprocessor,\n",
    "        \"Scaler\": scaler,\n",
    "        \"Sampler\": sampler,\n",
    "        \"Classifier\": classifier\n",
    "    })\n",
    "\n",
    "\n",
    "    # Fit and evaluate\n",
    "    pipe.fit(X_train, Y_train)\n",
    "    y_pred = pipe.predict(X_test)\n",
    "    y_prob = pipe.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    # Metrics\n",
    "    auc = roc_auc_score(Y_test, y_prob)\n",
    "    sex_eq_odds_diff = equalized_odds_difference(Y_test, y_pred, sensitive_features=sex_test)\n",
    "    race_eq_odds_diff = equalized_odds_difference(Y_test, y_pred, sensitive_features=race_test)\n",
    "    fg_eq_odds_diff = equalized_odds_difference(Y_test, y_pred, sensitive_features=fg_test)\n",
    "    income_eq_odds_diff = equalized_odds_difference(Y_test, y_pred, sensitive_features=income_test)\n",
    "    sex_race_eq_odds_diff = equalized_odds_difference(Y_test, y_pred, sensitive_features=sex_race_test)\n",
    "    fg_income_eq_odds_diff = equalized_odds_difference(Y_test, y_pred, sensitive_features=fg_income_test)\n",
    "\n",
    "    verse_dict = {\n",
    "    \"Include_Transfer\": 1 if include_transfer else 0,\n",
    "    \"Include_Covid\": 1 if include_covid else 0,\n",
    "    \"Handle_Nan\": handle_nan,\n",
    "    \"Train_Size\": train_size,\n",
    "    \"Include_Sex\": 1 if include_sex else 0,\n",
    "    \"Include_Race\": 1 if include_race else 0,\n",
    "    \"Encoder\": 'OneHotEncoder' if isinstance(encoder, OneHotEncoder) else 'OrdinalEncoder',\n",
    "    \"Scaler\": 'StandardScaler' if scaler else None,\n",
    "    \"Sampler\": sampler.__class__.__name__ if sampler else None,\n",
    "    \"Classifier\": classifier.__class__.__name__,\n",
    "    }\n",
    "\n",
    "    if isinstance(classifier, RandomForestClassifier):\n",
    "        verse_dict['N_Estimators'] = classifier.n_estimators\n",
    "    elif isinstance(classifier, GradientBoostingClassifier):\n",
    "        verse_dict['Learning_Rate'] = classifier.learning_rate\n",
    "    elif isinstance(classifier, LogisticRegression):\n",
    "        verse_dict['C'] = classifier.C\n",
    "\n",
    "    # Add the AUC and equalized odds difference after classifier parameters\n",
    "    verse_dict.update({\n",
    "        \"AUC\": auc,\n",
    "        \"Equalized_Odds_Difference_Sex\": sex_eq_odds_diff,\n",
    "        \"Equalized_Odds_Difference_Race\": race_eq_odds_diff,\n",
    "        \"Equalized_Odds_Difference_FG\": fg_eq_odds_diff,\n",
    "        \"Equalized_Odds_Difference_Income\": income_eq_odds_diff,\n",
    "        \"Equalized_Odds_Difference_Sex/Race\": sex_race_eq_odds_diff,\n",
    "        \"Equalized_Odds_Difference_FG/Income\": fg_income_eq_odds_diff\n",
    "    })\n",
    "    \n",
    "    # Ensure that the DataFrame has consistent columns\n",
    "    result_df = pd.DataFrame([verse_dict], columns=reenroll_multiverse_col_names)\n",
    "    save_header = not os.path.isfile('reenroll_partial_results.csv')\n",
    "\n",
    "    # Save the result after processing each combination\n",
    "    with open('reenroll_partial_results.csv', 'a') as f:\n",
    "        result_df.to_csv(f, header=save_header, index=False)\n",
    "    return verse_dict\n",
    "\n",
    "# Parallel execution\n",
    "results = Parallel(n_jobs=-1, backend=\"multiprocessing\")(\n",
    "    delayed(process_combination)(\n",
    "        include_transfer, include_covid, handle_nan, train_size, include_sex, include_race, encoder, scaler, sampler, classifier, classifier_params\n",
    "    )\n",
    "   for include_transfer in [True, False]\n",
    "    for include_covid in [True, False]\n",
    "    for handle_nan in [\"Impute\", \"Drop\"]\n",
    "    for train_size in [0.7, 0.8]\n",
    "    for include_sex in [True, False]\n",
    "    for include_race in [True, False]\n",
    "    for encoder in [OneHotEncoder(handle_unknown='ignore'), OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)]\n",
    "    for scaler in [StandardScaler(with_mean=False), None]\n",
    "    for sampler in [SMOTE(), NearMiss(), None]\n",
    "    for classifier, classifier_params in [\n",
    "        (RandomForestClassifier(), {\"n_estimators\": 50}),\n",
    "        (RandomForestClassifier(), {\"n_estimators\": 100}),\n",
    "        (RandomForestClassifier(), {\"n_estimators\": 150}),\n",
    "        (GradientBoostingClassifier(), {\"learning_rate\": 0.01}),\n",
    "        (GradientBoostingClassifier(), {\"learning_rate\": 0.1}),\n",
    "        (GradientBoostingClassifier(), {\"learning_rate\": 1}),\n",
    "        (LogisticRegression(max_iter=3500, solver='saga'), {\"C\": 0.01}),\n",
    "        (LogisticRegression(max_iter=3500, solver='saga'), {\"C\": 0.1}),\n",
    "        (LogisticRegression(max_iter=3500, solver='saga'), {\"C\": 1})\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Convert results to DataFrame and save to CSV\n",
    "reenroll_multiverse_df = pd.DataFrame(results)\n",
    "reenroll_multiverse_df.to_csv(\"reenroll_multiverse_result.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7e536547-cfe4-42c9-a06c-970551235d13",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    " for handle_nan in [\"Impute\"]:\n",
    "    if handle_nan == 'Drop':\n",
    "        rows_with_nan = features_df.isnull().any(axis=1)\n",
    "        X_after_nan = features_df[~rows_with_nan]\n",
    "        Y_after_nan = premature_Y[~rows_with_nan]\n",
    "    else:\n",
    "        imputer = SimpleImputer(strategy=\"most_frequent\")\n",
    "        X_after_nan_array = imputer.fit_transform(features_df)\n",
    "        X_after_nan = pd.DataFrame(X_after_nan_array, columns=features_df.columns, index=features_df.index)\n",
    "        Y_after_nan = premature_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0b197d62-3f20-4520-97d1-adfad49ccf1d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for train_size in [0.7, 0.8]:\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X_after_nan, Y_after_nan, train_size=train_size)\n",
    "    X_train = X_train.reset_index(drop=True)\n",
    "    X_test = X_test.reset_index(drop=True)\n",
    "    Y_train = Y_train.reset_index(drop=True)\n",
    "    Y_test = Y_test.reset_index(drop=True)\n",
    "    sex_test = X_test['STDNT_SEX_SHORT_DES']\n",
    "    race_test = X_test['STDNT_ETHNC_GRP_SHORT_DES']\n",
    "    for include_sex in [False, True]:\n",
    "\n",
    "\n",
    "        if not include_sex:\n",
    "            X_train_after_sex = X_train.drop(columns=['STDNT_SEX_SHORT_DES'])\n",
    "            X_test_after_sex = X_test.drop(columns=['STDNT_SEX_SHORT_DES'])\n",
    "        else:\n",
    "            X_train_after_sex = X_train\n",
    "            X_test_after_sex = X_test\n",
    "\n",
    "        for include_race in [False, True]:\n",
    "\n",
    "\n",
    "            if not include_race:\n",
    "                X_train_final = X_train_after_sex.drop(columns=['STDNT_ETHNC_GRP_SHORT_DES'])\n",
    "                X_test_final = X_test_after_sex.drop(columns=['STDNT_ETHNC_GRP_SHORT_DES'])\n",
    "            else:\n",
    "                X_train_final = X_train_after_sex\n",
    "                X_test_final = X_test_after_sex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2777e17a-6376-443c-a94a-a6def7ea449e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for encoder in [OneHotEncoder(handle_unknown='ignore')]:\n",
    "    categorical_columns = features_df.select_dtypes(include=['object']).columns\n",
    "    # Step 2: Create a ColumnTransformer with the current encoder\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('cat', encoder, categorical_columns)\n",
    "        ],\n",
    "        remainder='passthrough'  # Leave the rest of the columns as they are\n",
    "    )\n",
    "\n",
    "    pipe.set_params(**{\"Transformer\": preprocessor})\n",
    "    for scaler in [StandardScaler(with_mean=False)]:\n",
    "        pipe.set_params(**{\"Scaler\": scaler})\n",
    "        for sampler in [SMOTE()]:\n",
    "            pipe.set_params(**{\"Sampler\": sampler})\n",
    "            for classifier in [GradientBoostingClassifier()]:\n",
    "                pipe.set_params(**{\"Classifier\": classifier})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6929be73-6185-41c8-9b1f-b51afa3bb017",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[48], line 28\u001b[0m\n\u001b[1;32m     25\u001b[0m verse_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mClassifier\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGradientBoosting\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     26\u001b[0m verse_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLearning_Rate\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m n\n\u001b[0;32m---> 28\u001b[0m pipe\u001b[38;5;241m.\u001b[39mfit(X_train_final, Y_train)\n\u001b[1;32m     29\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m pipe\u001b[38;5;241m.\u001b[39mpredict(X_test_final)\n\u001b[1;32m     30\u001b[0m y_prob \u001b[38;5;241m=\u001b[39m pipe\u001b[38;5;241m.\u001b[39mpredict_proba(X_test_final)[:, \u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[0;32m/opt/conda/envs/haytham/lib/python3.12/site-packages/sklearn/base.py:1474\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1467\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1469\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1470\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1471\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1472\u001b[0m     )\n\u001b[1;32m   1473\u001b[0m ):\n\u001b[0;32m-> 1474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/conda/envs/haytham/lib/python3.12/site-packages/imblearn/pipeline.py:333\u001b[0m, in \u001b[0;36mPipeline.fit\u001b[0;34m(self, X, y, **params)\u001b[0m\n\u001b[1;32m    331\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_final_estimator \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassthrough\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    332\u001b[0m         last_step_params \u001b[38;5;241m=\u001b[39m routed_params[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m0\u001b[39m]]\n\u001b[0;32m--> 333\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_final_estimator\u001b[38;5;241m.\u001b[39mfit(Xt, yt, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mlast_step_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    334\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m/opt/conda/envs/haytham/lib/python3.12/site-packages/sklearn/base.py:1474\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1467\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1469\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1470\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1471\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1472\u001b[0m     )\n\u001b[1;32m   1473\u001b[0m ):\n\u001b[0;32m-> 1474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/conda/envs/haytham/lib/python3.12/site-packages/sklearn/ensemble/_gb.py:784\u001b[0m, in \u001b[0;36mBaseGradientBoosting.fit\u001b[0;34m(self, X, y, sample_weight, monitor)\u001b[0m\n\u001b[1;32m    781\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_resize_state()\n\u001b[1;32m    783\u001b[0m \u001b[38;5;66;03m# fit the boosting stages\u001b[39;00m\n\u001b[0;32m--> 784\u001b[0m n_stages \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_stages(\n\u001b[1;32m    785\u001b[0m     X_train,\n\u001b[1;32m    786\u001b[0m     y_train,\n\u001b[1;32m    787\u001b[0m     raw_predictions,\n\u001b[1;32m    788\u001b[0m     sample_weight_train,\n\u001b[1;32m    789\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_rng,\n\u001b[1;32m    790\u001b[0m     X_val,\n\u001b[1;32m    791\u001b[0m     y_val,\n\u001b[1;32m    792\u001b[0m     sample_weight_val,\n\u001b[1;32m    793\u001b[0m     begin_at_stage,\n\u001b[1;32m    794\u001b[0m     monitor,\n\u001b[1;32m    795\u001b[0m )\n\u001b[1;32m    797\u001b[0m \u001b[38;5;66;03m# change shape of arrays after fit (early-stopping or additional ests)\u001b[39;00m\n\u001b[1;32m    798\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_stages \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]:\n",
      "File \u001b[0;32m/opt/conda/envs/haytham/lib/python3.12/site-packages/sklearn/ensemble/_gb.py:880\u001b[0m, in \u001b[0;36mBaseGradientBoosting._fit_stages\u001b[0;34m(self, X, y, raw_predictions, sample_weight, random_state, X_val, y_val, sample_weight_val, begin_at_stage, monitor)\u001b[0m\n\u001b[1;32m    873\u001b[0m         initial_loss \u001b[38;5;241m=\u001b[39m factor \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_loss(\n\u001b[1;32m    874\u001b[0m             y_true\u001b[38;5;241m=\u001b[39my_oob_masked,\n\u001b[1;32m    875\u001b[0m             raw_prediction\u001b[38;5;241m=\u001b[39mraw_predictions[\u001b[38;5;241m~\u001b[39msample_mask],\n\u001b[1;32m    876\u001b[0m             sample_weight\u001b[38;5;241m=\u001b[39msample_weight_oob_masked,\n\u001b[1;32m    877\u001b[0m         )\n\u001b[1;32m    879\u001b[0m \u001b[38;5;66;03m# fit next stage of trees\u001b[39;00m\n\u001b[0;32m--> 880\u001b[0m raw_predictions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_stage(\n\u001b[1;32m    881\u001b[0m     i,\n\u001b[1;32m    882\u001b[0m     X,\n\u001b[1;32m    883\u001b[0m     y,\n\u001b[1;32m    884\u001b[0m     raw_predictions,\n\u001b[1;32m    885\u001b[0m     sample_weight,\n\u001b[1;32m    886\u001b[0m     sample_mask,\n\u001b[1;32m    887\u001b[0m     random_state,\n\u001b[1;32m    888\u001b[0m     X_csc\u001b[38;5;241m=\u001b[39mX_csc,\n\u001b[1;32m    889\u001b[0m     X_csr\u001b[38;5;241m=\u001b[39mX_csr,\n\u001b[1;32m    890\u001b[0m )\n\u001b[1;32m    892\u001b[0m \u001b[38;5;66;03m# track loss\u001b[39;00m\n\u001b[1;32m    893\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m do_oob:\n",
      "File \u001b[0;32m/opt/conda/envs/haytham/lib/python3.12/site-packages/sklearn/ensemble/_gb.py:490\u001b[0m, in \u001b[0;36mBaseGradientBoosting._fit_stage\u001b[0;34m(self, i, X, y, raw_predictions, sample_weight, sample_mask, random_state, X_csc, X_csr)\u001b[0m\n\u001b[1;32m    487\u001b[0m     sample_weight \u001b[38;5;241m=\u001b[39m sample_weight \u001b[38;5;241m*\u001b[39m sample_mask\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat64)\n\u001b[1;32m    489\u001b[0m X \u001b[38;5;241m=\u001b[39m X_csc \u001b[38;5;28;01mif\u001b[39;00m X_csc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m X\n\u001b[0;32m--> 490\u001b[0m tree\u001b[38;5;241m.\u001b[39mfit(\n\u001b[1;32m    491\u001b[0m     X, neg_g_view[:, k], sample_weight\u001b[38;5;241m=\u001b[39msample_weight, check_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    492\u001b[0m )\n\u001b[1;32m    494\u001b[0m \u001b[38;5;66;03m# update tree leaves\u001b[39;00m\n\u001b[1;32m    495\u001b[0m X_for_tree_update \u001b[38;5;241m=\u001b[39m X_csr \u001b[38;5;28;01mif\u001b[39;00m X_csr \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m X\n",
      "File \u001b[0;32m/opt/conda/envs/haytham/lib/python3.12/site-packages/sklearn/base.py:1474\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1467\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1469\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1470\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1471\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1472\u001b[0m     )\n\u001b[1;32m   1473\u001b[0m ):\n\u001b[0;32m-> 1474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/conda/envs/haytham/lib/python3.12/site-packages/sklearn/tree/_classes.py:1377\u001b[0m, in \u001b[0;36mDecisionTreeRegressor.fit\u001b[0;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[1;32m   1347\u001b[0m \u001b[38;5;129m@_fit_context\u001b[39m(prefer_skip_nested_validation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   1348\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, check_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m   1349\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Build a decision tree regressor from the training set (X, y).\u001b[39;00m\n\u001b[1;32m   1350\u001b[0m \n\u001b[1;32m   1351\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1374\u001b[0m \u001b[38;5;124;03m        Fitted estimator.\u001b[39;00m\n\u001b[1;32m   1375\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1377\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m_fit(\n\u001b[1;32m   1378\u001b[0m         X,\n\u001b[1;32m   1379\u001b[0m         y,\n\u001b[1;32m   1380\u001b[0m         sample_weight\u001b[38;5;241m=\u001b[39msample_weight,\n\u001b[1;32m   1381\u001b[0m         check_input\u001b[38;5;241m=\u001b[39mcheck_input,\n\u001b[1;32m   1382\u001b[0m     )\n\u001b[1;32m   1383\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m/opt/conda/envs/haytham/lib/python3.12/site-packages/sklearn/tree/_classes.py:472\u001b[0m, in \u001b[0;36mBaseDecisionTree._fit\u001b[0;34m(self, X, y, sample_weight, check_input, missing_values_in_feature_mask)\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    462\u001b[0m     builder \u001b[38;5;241m=\u001b[39m BestFirstTreeBuilder(\n\u001b[1;32m    463\u001b[0m         splitter,\n\u001b[1;32m    464\u001b[0m         min_samples_split,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    469\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_impurity_decrease,\n\u001b[1;32m    470\u001b[0m     )\n\u001b[0;32m--> 472\u001b[0m builder\u001b[38;5;241m.\u001b[39mbuild(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtree_, X, y, sample_weight, missing_values_in_feature_mask)\n\u001b[1;32m    474\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_outputs_ \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m is_classifier(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    475\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for n in [50, 100, 150]:\n",
    "    pipe.set_params(**{\"Classifier__n_estimators\": n})\n",
    "    verse_dict = {}\n",
    "    verse_dict[\"Include_Transfer\"] = 1 if include_transfer else 0\n",
    "    verse_dict[\"Include_Covid\"] = 1 if include_covid else 0\n",
    "    verse_dict[\"Handle_Nan\"] = handle_nan\n",
    "    verse_dict[\"Train_Size\"] = train_size\n",
    "    verse_dict[\"Include_Sex\"] = 1 if include_sex else 0\n",
    "    verse_dict[\"Include_Race\"] = 1 if include_race else 0\n",
    "\n",
    "    if isinstance(encoder, OneHotEncoder):\n",
    "        verse_dict['Encoder'] = 'OneHotEncoder'\n",
    "    elif isinstance(encoder, OrdinalEncoder):\n",
    "        verse_dict['Encoder'] = 'OrdinalEncoder'\n",
    "\n",
    "    if scaler:\n",
    "        verse_dict['Scaler'] = 'StandardScaler'\n",
    "\n",
    "    if isinstance(sampler, SMOTE):\n",
    "        verse_dict['Sampler'] = 'SMOTE'\n",
    "\n",
    "    elif isinstance(sampler, NearMiss):\n",
    "        verse_dict['Sampler'] = 'NearMiss'\n",
    "\n",
    "    verse_dict['Classifier'] = \"GradientBoosting\"\n",
    "    verse_dict['Learning_Rate'] = n\n",
    "\n",
    "    pipe.fit(X_train_final, Y_train)\n",
    "    y_pred = pipe.predict(X_test_final)\n",
    "    y_prob = pipe.predict_proba(X_test_final)[:, 1]\n",
    "\n",
    "    auc = roc_auc_score(y_true=Y_test, y_score=y_prob)\n",
    "    verse_dict['AUC'] = auc\n",
    "   \n",
    "\n",
    "    sex_eq_odds_diff = equalized_odds_difference(y_true=Y_test, y_pred=y_pred, sensitive_features=sex_test)\n",
    "    verse_dict[\"Equalized_Odds_Difference_Sex\"] = sex_eq_odds_diff\n",
    "   \n",
    "\n",
    "    race_eq_odds_diff = equalized_odds_difference(y_true=Y_test, y_pred=y_pred, sensitive_features=race_test)\n",
    "    verse_dict[\"Equalized_Odds_Difference_Race\"] = race_eq_odds_diff\n",
    "   \n",
    "\n",
    "    new_row_df = pd.DataFrame([verse_dict])\n",
    "    reenroll_multiverse_df = pd.concat([reenroll_multiverse_df, new_row_df], ignore_index=True)\n",
    "print(reenroll_multiverse_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d3dc0851-cbeb-4dca-a560-efb75e44733a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        0\n",
      "1        0\n",
      "2        0\n",
      "3        0\n",
      "4        0\n",
      "        ..\n",
      "21876    0\n",
      "21877    1\n",
      "21878    0\n",
      "21879    0\n",
      "21880    1\n",
      "Name: STDNT_INTL_IND, Length: 21881, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(X_test_final.iloc[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "id": "cafaba6e-3e3f-4e33-9c1d-d33ed3d52043",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "202489    222323\n",
      "35598     124976\n",
      "190300     10943\n",
      "103542    249434\n",
      "164735    118447\n",
      "           ...  \n",
      "51814      74052\n",
      "34514     124757\n",
      "191939       496\n",
      "180844    345495\n",
      "150054    213247\n",
      "Name: STDNT_ID, Length: 45938, dtype: object\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "6eb2666f-d21e-4648-8af7-7cc36e095b96",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113578    0\n",
      "17349     0\n",
      "86978     0\n",
      "200966    0\n",
      "23233     0\n",
      "         ..\n",
      "81958     0\n",
      "107334    0\n",
      "46826     0\n",
      "404045    0\n",
      "4006      0\n",
      "Name: IS_REENROLL, Length: 99, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(Y_test[Y_test==0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "91d49bae-675f-463e-b68b-e4de1348ae29",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        STDNT_ID  STDNT_INTL_IND  STDNT_NTV_ENG_SPKR_IND  \\\n",
      "6         352207               0                       0   \n",
      "7          44685               0                       1   \n",
      "8         339010               0                       1   \n",
      "9         301099               0                       0   \n",
      "10         46601               0                       1   \n",
      "...          ...             ...                     ...   \n",
      "198560     86047               0                       0   \n",
      "198562    383383               0                       1   \n",
      "198564    395358               0                       1   \n",
      "198571    290628               0                       1   \n",
      "198572    105012               0                       1   \n",
      "\n",
      "       FIRST_US_PRMNNT_RES_PSTL_5_CD     PRNT_MAX_ED_LVL_DES  \\\n",
      "6                              17110  Professional Doctorate   \n",
      "7                              49085  Professional Doctorate   \n",
      "8                              22207            Some College   \n",
      "9                              06518       Bachelor's degree   \n",
      "10                             43566       Bachelor's degree   \n",
      "...                              ...                     ...   \n",
      "198560                         48323         Master's degree   \n",
      "198562                         49103               Doctorate   \n",
      "198564                         07090  Professional Doctorate   \n",
      "198571                         48631      Associate's degree   \n",
      "198572                         48197         Master's degree   \n",
      "\n",
      "       FIRST_TERM_ATTND_SHORT_DES  STARTING_AGE  IS_FIRST_GEN  \\\n",
      "6                         FA 2019          18.0           0.0   \n",
      "7                         FA 2010          18.0           0.0   \n",
      "8                         FA 2018          18.0           0.0   \n",
      "9                         FA 2012          18.0           0.0   \n",
      "10                        FA 2014          19.0           0.0   \n",
      "...                           ...           ...           ...   \n",
      "198560                    FA 2016          18.0           0.0   \n",
      "198562                    SU 2021          18.0           0.0   \n",
      "198564                    FA 2021          18.0           0.0   \n",
      "198571                    FA 2013          18.0           0.0   \n",
      "198572                    FA 2014          19.0           0.0   \n",
      "\n",
      "        TRANSFER_CREDITS  MAX_SAT_SCR EST_GROSS_FAM_INC_DES  FIRST_YR_CUM_GPA  \\\n",
      "6                    4.0       1420.0     $50,000 - $74,999             3.748   \n",
      "7                    3.0       1340.0     $50,000 - $74,999             2.829   \n",
      "8                    3.0       1310.0   $100,000 - $149,999             3.376   \n",
      "9                    5.0       1370.0   $100,000 - $149,999             3.468   \n",
      "10                   4.0       1500.0   $100,000 - $149,999             3.525   \n",
      "...                  ...          ...                   ...               ...   \n",
      "198560               4.0       1540.0    More than $200,000             2.738   \n",
      "198562               3.0       1250.0   $150,000 - $199,999             3.700   \n",
      "198564               1.0       1390.0            Don't Know             4.000   \n",
      "198571               4.0       1110.0     $75,000 - $99,999             2.203   \n",
      "198572               4.0       1400.0   $100,000 - $149,999             3.203   \n",
      "\n",
      "        FIRST_YR_TAKEN_CREDITS  HS_GPA  HS_CALC_IND  HS_CHEM_LAB_IND  \\\n",
      "6                         31.0    3.90            0                0   \n",
      "7                         36.0    3.76            1                1   \n",
      "8                         30.0    3.70            1                1   \n",
      "9                         16.0    3.80            1                1   \n",
      "10                        31.0    4.00            1                1   \n",
      "...                        ...     ...          ...              ...   \n",
      "198560                    33.0    3.60            1                1   \n",
      "198562                     6.0    3.90            1                0   \n",
      "198564                    18.0    3.80            1                0   \n",
      "198571                    27.0    2.90            0                0   \n",
      "198572                    29.0    3.80            1                0   \n",
      "\n",
      "                                    PGM_1_MAJOR_1_CIP_DES  \\\n",
      "6       Liberal Arts and Sciences, General Studies and...   \n",
      "7       Liberal Arts and Sciences, General Studies and...   \n",
      "8       Liberal Arts and Sciences, General Studies and...   \n",
      "9       Liberal Arts and Sciences, General Studies and...   \n",
      "10      Liberal Arts and Sciences, General Studies and...   \n",
      "...                                                   ...   \n",
      "198560                               Engineering, General   \n",
      "198562        Sport and Fitness Administration/Management   \n",
      "198564  Liberal Arts and Sciences, General Studies and...   \n",
      "198571            Multi-/Interdisciplinary Studies, Other   \n",
      "198572  Liberal Arts and Sciences, General Studies and...   \n",
      "\n",
      "       FIRST_UG_ENTRY_TYP_DES  \n",
      "6                  First-Year  \n",
      "7                  First-Year  \n",
      "8                  First-Year  \n",
      "9                  First-Year  \n",
      "10                 First-Year  \n",
      "...                       ...  \n",
      "198560             First-Year  \n",
      "198562             First-Year  \n",
      "198564             First-Year  \n",
      "198571             First-Year  \n",
      "198572             First-Year  \n",
      "\n",
      "[80155 rows x 18 columns]\n"
     ]
    }
   ],
   "source": [
    "test = add_major_df[['STDNT_ID', 'STDNT_INTL_IND', 'STDNT_NTV_ENG_SPKR_IND', 'FIRST_US_PRMNNT_RES_PSTL_5_CD', 'PRNT_MAX_ED_LVL_DES', 'FIRST_TERM_ATTND_SHORT_DES',\n",
    "                      'STARTING_AGE', 'IS_FIRST_GEN', 'TRANSFER_CREDITS', 'MAX_SAT_SCR', 'EST_GROSS_FAM_INC_DES', 'FIRST_YR_CUM_GPA', \n",
    "                      'FIRST_YR_TAKEN_CREDITS', 'HS_GPA', 'HS_CALC_IND', 'HS_CHEM_LAB_IND', 'PGM_1_MAJOR_1_CIP_DES', 'FIRST_UG_ENTRY_TYP_DES']]\n",
    "print(test.dropna())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "65ea410d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6         1\n",
      "7         1\n",
      "8         1\n",
      "9         0\n",
      "10        1\n",
      "         ..\n",
      "198560    1\n",
      "198562    1\n",
      "198564    1\n",
      "198571    1\n",
      "198572    1\n",
      "Name: IS_REENROLL, Length: 80155, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "test = test.dropna()\n",
    "target_df = generate_reenroll_target(test, term_info_df)\n",
    "print(target_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "1feac6fe",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Re-Enroll First-Year:  79035\n",
      "Numer of Not Re-Enroll First-Year:  1120\n"
     ]
    }
   ],
   "source": [
    "sd_id_1 = target_df[target_df == 1]\n",
    "sd_id_0 = target_df[target_df == 0]\n",
    "print(\"Number of Re-Enroll First-Year: \", len(sd_id_1))\n",
    "print(\"Numer of Not Re-Enroll First-Year: \", len(sd_id_0))\n",
    "# match = term_info_df[term_info_df['STDNT_ID'] == 367959]\n",
    "# print(match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "85c122d4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# income_df = fill_income_col(relevant_student_df, 'zip_mode')\n",
    "# print(income_df[income_df['STDNT_ID'] == 250287][['EST_GROSS_FAM_INC_DES']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "64c38c92",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# add_age_df = add_age_col(relevant_student_df)\n",
    "# print(add_age_df[add_age_df['STARTING_AGE'] > 90][['FIRST_TERM_ATTND_BEGIN_YR_MO', 'STDNT_BIRTH_YR']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "bdf2f407",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# add_transfer_credits_df = add_transfer_credits_col(relevant_student_df, transfer_info_df)\n",
    "# print(add_transfer_credits_df['TRANSFER_CREDITS'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "16326b14",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# add_max_sat_df = add_max_sat_col(relevant_student_df)\n",
    "# print(add_max_sat_df['MAX_SAT_SCR'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "c892816f-6495-4a11-9389-5d879df58a6e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# #student_term_info_df\n",
    "# print(term_info_df[term_info_df['PGM_1_MAJOR_1_CIP_DES'].isna()])\n",
    "# print(\"####\")\n",
    "\n",
    "# # print(term_info_df[term_info_df['TERM_CD'] == 2420])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "600ee17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(term_info_df[term_info_df['STDNT_ID'] == 413831][['PGM_1_MAJOR_1_CIP_DES']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f92a22-2065-4f2b-89a1-56d346d2b6c4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# student_info_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c7579f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(student_info_df[(student_info_df['FIRST_UG_ENTRY_TYP_DES'] == 'First-Year') | (student_info_df['FIRST_UG_ENTRY_TYP_DES'] == 'First-Year Assumed')]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6699df-bbb9-4d2b-a763-e20ecfd90c7d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# class_info_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f5e77c-1998-433f-a2c1-234db1999d38",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# transfer_info_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "modules",
   "language": "python",
   "name": "haytham"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
